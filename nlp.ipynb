{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KSG722/class2022Spring/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltUO0sYwyGfU"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di6xZ08xsgO7"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어처리에 필요한 기본적인 패키지\n",
        "\n",
        "natural language tokenization인듯\n",
        "\n",
        "tokenization : 긴 단어를 string별로 끊어서 token화하는 것"
      ],
      "metadata": {
        "id": "HfSsWblb7nLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy a file from github\n",
        "import os\n",
        "url = \"https://raw.githubusercontent.com/hsnam95/class2022Spring/main/crime_punishment.txt\"\n",
        "os.system(\"curl \" + url + \" > crime_punishment.txt\")"
      ],
      "metadata": {
        "id": "btgs9Nt-2Yj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 내려받아 깃텁에 다운로드"
      ],
      "metadata": {
        "id": "dmVgtyKvP6x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read a text file in the server\n",
        "file = open(\"crime_punishment.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "j854H5tZP5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 오픈"
      ],
      "metadata": {
        "id": "inKzkBnHP824"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbHEyyNHntcZ"
      },
      "source": [
        "# or copy/pase text here\n",
        "text = 'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently — they’re not fond of rules. \\\n",
        "You can quote them, disagree with them, glorify or vilify them, \\\n",
        "but the only thing you can’t do is ignore them because they change things. \\\n",
        "They push the human race forward, and while some may see them as the crazy ones, we see genius, \\\n",
        "because the ones who are crazy enough to think that they can change the world, are the ones who do.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"tmp.txt\", \"w\")\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "9saTcc9C4Cjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45jV2UYs1GEC"
      },
      "source": [
        "text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "뛰어쓰기를 기반으로 텍스트 쪼개기. 기본적인 tokenization."
      ],
      "metadata": {
        "id": "aIHI7XfJQShK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFR-cRaahTPy"
      },
      "source": [
        "' '.join(text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트를 띄어쓰기 단위(' ')로 결합시키기\n",
        "\n",
        "만약 띄어쓰기 말고 ''이라면 띄어쓰기 없이 쭈욱 결합됨"
      ],
      "metadata": {
        "id": "twTgHoqsQW1D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq_lPZMHntcb"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "words = word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "punkt와 띄어쓰기를 단위로 쪼개기. -> punctuation을 살림"
      ],
      "metadata": {
        "id": "TSa_T3jdQq6a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC1fe7nWF6wN"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\")\n",
        "words = retokenize.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegexpTokenizer : puncuation도 없이 쪼개기. 즉 온점, 반점 등 없이 오직 string만 남게됨."
      ],
      "metadata": {
        "id": "1DMpmlamQzHK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc42Plwx56YS"
      },
      "source": [
        "### Normalization  \n",
        "\n",
        "접두사 등을 분리해서 깎아 내는 것\n",
        "\n",
        "**Stemming** 어간 추출, 대충의 패턴 규칙으로 어미를 잘라내어 어간만 남기기 (사전에 없는 어간 나올 수 있음) \n",
        "\n",
        "예시) purify에서 fy 자르고 puri만 남기기\n",
        "\n",
        "**Lemmatization** 표제어(기본 사전형)를 기반으로 원형을 추출.\n",
        "\n",
        "예시) puri에서 purify라는 원형 도출하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsFfoAr259Fs"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkbgNiPd8BdL"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIh5pYd8f74"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPgIzrjm8_1N"
      },
      "source": [
        "### Stopword"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "너무 많이 쓰이는 단어 : 조동사 , 관사 -> 별로 쓰잘데 없이 자주 나오는 단어 -> 따로 모아놓은 것을 stopword라고 함"
      ],
      "metadata": {
        "id": "GwJIdTXvLsFQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdM2FaN8ntcc"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "nltk.download('stopwords')\n",
        "print(words)\n",
        "words = [w for w in words if not w in stopwords.words('english')]\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmwXTL0UA5aw"
      },
      "source": [
        "### Collocation, Concordance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fll4ygxNA3OJ"
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "words = retokenize.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVXlhIrAtmf"
      },
      "source": [
        "nltk.Text(words).collocations()  # default: (num=20, window_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "총 20개의 연음어찾기. 단어는 2개짜리. "
      ],
      "metadata": {
        "id": "hc5VEMc9S_we"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq0wiutwA_au"
      },
      "source": [
        "nltk.Text(words).concordance('Emma', 79, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "용례 만들기. Emma라는 단어가 등장하는 용례 찾기. 79는 문장 당 알파벳 개수, 용례의 개수 10개"
      ],
      "metadata": {
        "id": "Wwsb_TNlS848"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIAIhXvP_BjU"
      },
      "source": [
        "nltk.Text(words).dispersion_plot([\"Emma\", \"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 단어들이 많이 나왔는지 시각화."
      ],
      "metadata": {
        "id": "58heGoqpT20i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWYZOFxq_ex2"
      },
      "source": [
        "# Distributional similarity: \n",
        "# find other words which appear in the same contexts as the specified word; \n",
        "# list most similar words first.\n",
        "nltk.Text(words).similar(\"Emma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "emma와 비슷하게 양 쪽 옆으로 나올 단어들을 찾아라. 예를 들어 Emma는 여자기에 she가 자주 주변에 위치."
      ],
      "metadata": {
        "id": "mp2ouounUPBZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZihiVSBK_vy7"
      },
      "source": [
        "# Find contexts where the specified words appear; list most frequent common contexts first.\n",
        "nltk.Text(words).common_contexts([\"Emma\", \"she\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emma와 she라는 specified words가 나타는 문장을 찾아내라."
      ],
      "metadata": {
        "id": "tfk-z20oUVWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "collocation : 연어. 무조건 같이 나오는 단어들. take care 혹은 michael smith\n",
        "\n",
        "concordance : 특정 단어가 특정 문맥에서 사용되는 용례를 찾아주는 것"
      ],
      "metadata": {
        "id": "YD9FhtC_L0xU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8TrCE14vGcT"
      },
      "source": [
        "### Frequency distribution, Frequency plot\n",
        "\n",
        "frenquency : 어떤 단어가 얼마나 많이 사용되었는지. 어떤 document에서 특정 단어가 많이 사용될 경우 어떤 문서인지 예측 가능 -> pitcher라는 단어의 빈도가 높으면, 야구와 관련된 text인것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdY3m6zSBHic"
      },
      "source": [
        "fd = nltk.FreqDist(words).most_common(20)\n",
        "fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tpZThNV-ftv"
      },
      "source": [
        "nltk.Text(words).plot(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSOSzIovvKvE"
      },
      "source": [
        "### Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIcAOAvqntce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4f7b49-0a89-4728-8d1e-28afcf6a04b7"
      },
      "source": [
        "nltk.download('words')\n",
        "nltk.corpus.words.words('en')[-20:-1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['zymosis',\n",
              " 'zymosterol',\n",
              " 'zymosthenic',\n",
              " 'zymotechnic',\n",
              " 'zymotechnical',\n",
              " 'zymotechnics',\n",
              " 'zymotechny',\n",
              " 'zymotic',\n",
              " 'zymotically',\n",
              " 'zymotize',\n",
              " 'zymotoxic',\n",
              " 'zymurgy',\n",
              " 'Zyrenian',\n",
              " 'Zyrian',\n",
              " 'Zyryan',\n",
              " 'zythem',\n",
              " 'Zythia',\n",
              " 'zythum',\n",
              " 'Zyzomys']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk.corpus.words.words('en')[-20:-1] : 영단어 사전 중 마지막 20개 보기"
      ],
      "metadata": {
        "id": "6yU_GVFqUrAw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjAy_Ju7ntce"
      },
      "source": [
        "len(nltk.corpus.words.words('en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyIIqwosCRZa"
      },
      "source": [
        "### Extract information (pos tag, named entity)\n",
        "\n",
        "pos tag : 품사 태그. 어떤 품사인지 찾아서 태그를 붙임\n",
        "named entity : 품사 외의 정보들. 예를 들어 london -> location, David -> name 과 같이 해당 단어가 나타내는 정보의 종류 등을 표시해줌."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **POS tag list**:\n",
        "\n",
        "CC\tcoordinating conjunction \\\n",
        "CD\tcardinal digit \\\n",
        "DT\tdeterminer \\\n",
        "EX\texistential there (like: \"there is\" ... think of it like \"there exists\") \\\n",
        "FW\tforeign word \\\n",
        "IN\tpreposition/subordinating conjunction \\\n",
        "JJ\tadjective\t'big' \\\n",
        "JJR\tadjective, comparative\t'bigger' \\\n",
        "JJS\tadjective, superlative\t'biggest' \\\n",
        "LS\tlist marker\t1) \\\n",
        "MD\tmodal\tcould, will \\\n",
        "NN\tnoun, singular 'desk' \\\n",
        "NNS\tnoun plural\t'desks' \\\n",
        "NNP\tproper noun, singular\t'Harrison' \\\n",
        "NNPS\tproper noun, plural\t'Americans' \\\n",
        "PDT\tpredeterminer\t'all the kids' \\\n",
        "POS\tpossessive ending\tparent's \\\n",
        "PRP\tpersonal pronoun\tI, he, she \\\n",
        "PRP\\$\tpossessive pronoun\tmy, his, hers \\\n",
        "RB\tadverb\tvery, silently, \\\n",
        "RBR\tadverb, comparative\tbetter \\\n",
        "RBS\tadverb, superlative\tbest \\\n",
        "RP\tparticle\tgive up \\\n",
        "TO\tto\tgo 'to' the store. \\\n",
        "UH\tinterjection\terrrrrrrrm \\\n",
        "VB\tverb, base form\ttake \\\n",
        "VBD\tverb, past tense\ttook \\\n",
        "VBG\tverb, gerund/present participle\ttaking \\\n",
        "VBN\tverb, past participle\ttaken \\\n",
        "VBP\tverb, sing. present, non-3d\ttake \\\n",
        "VBZ\tverb, 3rd person sing. present\ttakes \\\n",
        "WDT\twh-determiner\twhich \\\n",
        "WP\twh-pronoun\twho, what \\\n",
        "WP\\$\tpossessive wh-pronoun\twhose \\\n",
        "WRB\twh-abverb\twhere, when \\"
      ],
      "metadata": {
        "id": "NLz0kp4kekaZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VBiObftCVwH"
      },
      "source": [
        "sent = \"I am Jhon from America and would like to go to Starbuck\"\n",
        "words = nltk.word_tokenize(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 문장을 tokenizae 먼저 하고"
      ],
      "metadata": {
        "id": "G1iaHewdU3Ev"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwKdu36WCewv"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = nltk.pos_tag(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰별로 품사 분류"
      ],
      "metadata": {
        "id": "-q-LuM3DU9hY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnjGT1HpClE0"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "NE = nltk.ne_chunk(pos)\n",
        "# common Entity types: ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9DEIZ4lXQF"
      },
      "source": [
        "### Wordcloud\n",
        "\n",
        "단어의 빈도를 visualization해주는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jypxOnw9hoyZ"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "\n",
        "wc = WordCloud().generate(text) \n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6xv5ClAl5xk"
      },
      "source": [
        "stopwords = set(STOPWORDS) \n",
        "stopwords.add('unto')\n",
        "wc = WordCloud(stopwords = stopwords).generate(text) \n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrVGVc0X9j7r"
      },
      "source": [
        "### Regular expression\n",
        "\n",
        "(중요)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKgoQFI_cG-"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "regular expression에 필요한 패키지"
      ],
      "metadata": {
        "id": "ScWpyGG57uDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.search('a', 'abcdefa')"
      ],
      "metadata": {
        "id": "qM5Uv5IyLeLG",
        "outputId": "90df3007-1ae8-4eaa-b44d-0b5686d3c11e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "주어진 string을 찾는 search함수. 2nd argument에서 1st argument를 찾으라는 것.\n",
        "\n",
        "span=(0,1) : a를 0번 원소(첫번째 원소)부터 1번 원소(두번째 원소) 사이에서 찾았다는 것. 첫번째 a만 찾는 것.\n"
      ],
      "metadata": {
        "id": "b4ndrr5L7w6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.search('bc','abcdefa')"
      ],
      "metadata": {
        "id": "RKRDQvxxrWtR",
        "outputId": "1858abd8-dc1e-4796-9e60-fb680a70ab79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(1, 3), match='bc'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "span=(1,3) : bc를 1번 원소(2번째 원소)부터 3번 원소(4번째 원소)째 사이에서 찾았다는 것\n",
        "즉 span=(n, m)은 찾고자 하는 값이 n번째 원소와 **m-1**원소에 있다는 것"
      ],
      "metadata": {
        "id": "q6Z6YTojrY8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('a', 'abcdefa')"
      ],
      "metadata": {
        "id": "KSVsBeO7LyOK",
        "outputId": "96a39827-342f-4981-82e8-e91a3a424e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "search와 달리 모든 'a'를 찾아서 list화."
      ],
      "metadata": {
        "id": "mZ4zQWax8Kh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub('a', 'b', 'abcdefa')"
      ],
      "metadata": {
        "id": "BSCfOsKuLzGi",
        "outputId": "8e651aad-a1bf-4db9-8c2b-93bc32616910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bbcdefb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "substitute의 약자. abcdefa라는 대상에서 a를 b로 바꾸어라. 참고로 모든 a를 b로 바꿈."
      ],
      "metadata": {
        "id": "xpRh979kAJEV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U2pS-NL9p38",
        "outputId": "d44d5e3d-9028-4156-be88-0845a058f8cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "'''       Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\n",
        "\n",
        ".\t        Wildcard, matches any character\n",
        "^abc\t    Matches some pattern abc at the start of a string\n",
        "abc$\t    Matches some pattern abc at the end of a string\n",
        "[abc]\t    Matches one of a set of characters\n",
        "[^abc]    Matches anything but a set of characters\n",
        "[A-Z0-9]\tMatches one of a range of characters\n",
        "ed|ing|s\tMatches one of the specified strings (disjunction)\n",
        "*\t        Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
        "+\t        One or more of previous item, e.g. a+, [a-z]+\n",
        "?\t        Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
        "{n}\t      Exactly n repeats where n is a non-negative integer\n",
        "{n,}\t    At least n repeats\n",
        "{,n}\t    No more than n repeats\n",
        "{m,n}\t    At least m and no more than n repeats\n",
        "a(b|c)+\t  Parentheses that indicate the scope of the operators\n",
        "(...)     Matches whatever regular expression is inside the parentheses\n",
        "\\d\n",
        "Matches any decimal digit; this is equivalent to the class [0-9].\n",
        "\\D\n",
        "Matches any non-digit character; this is equivalent to the class [^0-9].\n",
        "\\s\n",
        "Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v].\n",
        "\\S\n",
        "Matches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\f\\v].\n",
        "\\w\n",
        "Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
        "\\W\n",
        "Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'       Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\\n\\n.\\t        Wildcard, matches any character\\n^abc\\t    Matches some pattern abc at the start of a string\\nabc$\\t    Matches some pattern abc at the end of a string\\n[abc]\\t    Matches one of a set of characters\\n[^abc]    Matches anything but a set of characters\\n[A-Z0-9]\\tMatches one of a range of characters\\ned|ing|s\\tMatches one of the specified strings (disjunction)\\n*\\t        Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\\n+\\t        One or more of previous item, e.g. a+, [a-z]+\\n?\\t        Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\\n{n}\\t      Exactly n repeats where n is a non-negative integer\\n{n,}\\t    At least n repeats\\n{,n}\\t    No more than n repeats\\n{m,n}\\t    At least m and no more than n repeats\\na(b|c)+\\t  Parentheses that indicate the scope of the operators\\n(...)     Matches whatever regular expression is inside the parentheses\\n\\\\d\\nMatches any decimal digit; this is equivalent to the class [0-9].\\n\\\\D\\nMatches any non-digit character; this is equivalent to the class [^0-9].\\n\\\\s\\nMatches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\x0c\\x0b].\\n\\\\S\\nMatches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\x0c\\x0b].\\n\\\\w\\nMatches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\\n\\\\W\\nMatches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp3_Dm9Q_tNQ",
        "outputId": "b9b3689b-8677-4c76-d075-8b72334004b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "nltk.download('words')\n",
        "engdict = nltk.corpus.words.words('en')\n",
        "engdict[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk에서 사전 파일을 다운로드받고\n",
        "\n",
        "그 중에서 en(glish)에 해당하는 words들을 engdict로 설정\n",
        "\n",
        "타입은 리스트, 원소의 개수는 약 235886개, 첫번째 원소는 'A'\n",
        "\n",
        "이제 이 원소들을 대상으로 search와 findall함수 적용"
      ],
      "metadata": {
        "id": "Hfv68EtasN5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# result = [w for w in engdict if re.search('ed$', w)]\n",
        "# result = [w for w in engdict if re.search('^..j..t..$', w)]\n",
        "# result = [w for w in engdict if re.search('^[ghi][mno][jlk][def]$', w)]\n",
        "result = [w for w in engdict if re.search('^[ah]+$', w)][:10]\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "NpVF4ItRuOeq",
        "outputId": "659e3708-f1c6-4e2d-ea6c-2c4e3731f1aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'aa', 'ah', 'aha', 'h', 'ha', 'hah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "w for w in engdict : engdict에 있는 원소가 각각 w에 들어가 돌고\n",
        "if re.search('ed$',w)의 조건 제한을 한 번 돌 때마다 받음.\n",
        "\n",
        "engdict의 원소들을 차례로 w에 집어넣고 여기서 ed$라는 조건에 맞는 원소들을 찾으라는 것.  ed$ 가 바로 나름대로의 규칙이고 regular expression. 이 ed$가 있으면 if문을 통과해서 w에 안착하는 것\n",
        "\n",
        "참고로 $는 string의 끝에서 ed가 있는지를 묻는것(위에 빨간색글씨들 참고) -> ^ed였으면 stirng의 맨 앞에 ed가 있는지를 묻는 것\n",
        "\n",
        "예를들어 re.search('ed$','educated')가 있다면\n",
        "맨 앞의 ed가 아니라 맨 끝에서 ed를 가져오는 것. 이처럼 맨 뒤에 ed가 있으면 w에 남을 수 잇는 것.\n",
        "\n",
        "print(result[:10])로 reslut 중 뒤에 10개 원소만 가져오면 위와 같다\n",
        "\n",
        "\n",
        "두번째 줄 ^..j..t..$는 xxjxxtxx로 시작하고 끝나는(즉 그대로 이루어진) 단어를 찾으라는 것\n",
        "\n",
        "세번째 줄 [ghi]는 g, h, i 중 하나만 만족하면 된다는 것.\n",
        "따러서 gold, golf 등의 단어가 도출됨\n",
        "\n",
        "네번째줄 +는 한번 이상 나와야한다는 것.a 혹은 h가 한 개 이상 나와야함."
      ],
      "metadata": {
        "id": "Jqk9152VtPCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "id": "C61TfINfw3Yr",
        "outputId": "8d287b89-3dad-4d54-fef6-5cdc07ec91f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아까 len(engdict)는 23만개였는데 len(result)은 9192개. ed로 끝나는 단어가 9192개 정도라는 것."
      ],
      "metadata": {
        "id": "uF_XT-wMw5Ul"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1a5mQYj4hwn",
        "outputId": "95ea83f1-37c2-4b43-ba13-acb8e1c1c2e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('treebank')\n",
        "wsj = nltk.corpus.treebank.words()\n",
        "\n",
        "result = [w for w in wsj if re.search('(ed|ing)$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
        "# result = [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
        "# result = [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
        "\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', 'Alfred', 'Allied', 'Annualized', 'Anything']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "treebank는 신문에 나온 용어들을 모아놓은 것.\n",
        "\n",
        "(ed|ing)$ : ed 혹은 ing 중 둘 중 하나로 끝나는 단어를 도출하라는 것. 괄호는 그냥 ed와 ing을 묶어주는 것.\n",
        "\n",
        "^[0-9]+\\\\.[0-9]+$ : 숫자가 하나 이상 나오고 \\.은 진짜 온점(그냥 .은 미지수 x. 아무거나 들어가도 된다는 뜻). 그리고 숫자 하나 이상으로 끝나야 한다는 것. 즉 소수점을 가진 수. 전화번호나 이메일같은 특정한 규칙을 가진 string 데이터들은 이 방법을 사용하면 된다.\n",
        "\n",
        "^[A-Z]+\\$\\$ : 맨 뒤의 \\$은 끝을 의미. 그 앞의 \\$은 진짜 달러화 표시. 즉 대문자 하나 이상과 $(달러화)로 시작하고 끝나는 단어\n",
        "\n",
        "^[0-9]{4}$ : 숫자가 4번 나오는 단어\n",
        "\n",
        "[0-9]+-[a-z]{3,5}$ : 숫자가 하나이상 나오고 '-'가 있고 소문자가 3개 이상 5개 이하로 나오는 단어를 찾아라\n",
        "\n",
        "^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$ : 소문자가 5개 이상 나오고 '-'가 나오고 소문자가 2개에서 3개 나오고 '-'가 나오고 소문자가 6개 이하로 나오는 string을 찾아라 -> ex) father-in-law\n",
        "\n",
        "sorted는 정렬하는 함수. 원래는 신문 기사 순서대로 단어가 마구 나오는데, 오름차순으로 정렬해서 보여줌.\n",
        "\n",
        "\n",
        "이렇듯 search은 여러 개의 string을 걸러서 조건에 맞는 단어만 걸러내주는 함수. 하나를 찾아주는 함수.\n",
        "\n",
        "반면 findall은 긴 텍스트에서 조건에 맞는 단어를 list화 해주는 함수. 패턴에 알맞는 모든 데이터를 list화"
      ],
      "metadata": {
        "id": "lkvxX1qsx_5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "url = \"https://raw.githubusercontent.com/hsnam95/my/main/friends_season01_script.txt\"\n",
        "os.system(\"curl \" + url + \" > friends_season01_script.txt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "05zHW52eKatf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebf7fbd-5706-4804-be09-338d6c56c91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "friends 시트콤 대본 텍스트 불러오기.\n",
        "0 : 잘 불러와졌다는 뜻"
      ],
      "metadata": {
        "id": "NCniHzB_2Qfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read a text file in the server\n",
        "file = open(\"friends_season01_script.txt\")\n",
        "text = file.read()\n",
        "file.close()\n",
        "text"
      ],
      "metadata": {
        "id": "xYd9iJtw2PIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = '(?<=: ).+(?=[\\.|\\?|\\!])'\n",
        "sent = re.findall(pattern, text)\n",
        "sent"
      ],
      "metadata": {
        "id": "cBzwwwztZx9r",
        "outputId": "a043825f-19f5-44f1-cb3e-540776b1c72e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The previously unseen parts of this episode are shown in blue text',\n",
              " 'Central Perk, Chandler, Joey, Phoebe, and Monica are there',\n",
              " \"There's nothing to tell! He's just some guy I work with\",\n",
              " \"C'mon, you're going out with the guy! There's gotta be something wrong with him\",\n",
              " 'All right Joey, be nice.Ê So does he have a hump? A hump and a hairpiece',\n",
              " 'Wait, does he eat chalk',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 설정한 pattern은 유용한 패턴\n",
        "\n",
        "(?<=x).(?=y)는 조건 설정. 앞쪽엔 x가 있고 중간에 미지수(.)가 있고 뒷쪽에는 y가 있어야한다는 것.\n",
        "\n",
        "(?<=: ).+(?=[\\.|\\?|\\!]) : 왼쪽에는 :와 blank가 있고, 이후 미지수(.)가 하나 이상(+) 있고 그 오른쪽에 온점 혹은 물음표 혹은 느낌표 중 하나가 있어야 한다는 것.\n",
        "\n",
        "\n",
        "이 패턴을 이 text에 적용하여 패턴에 맞는 스크립트를 list화.\n",
        "이렇게 findall을 하면 : 부터 찾아와지기 때문에 등장인물의 이름이 없어지고 문장만 찾아와진 채 리스트화된다.\n"
      ],
      "metadata": {
        "id": "92kWUYN23jp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '\\n'.join(sent)\n",
        "text"
      ],
      "metadata": {
        "id": "dSfYZdMC4qyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "줄바꿈(\\n)을 기준으로 sent의 요소들을 다 이어붙이라는 명렁어"
      ],
      "metadata": {
        "id": "r7Kkj3824uF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"friends_season01_script.txt\")\n",
        "text1 = file.read()\n",
        "file.close()\n",
        "text1"
      ],
      "metadata": {
        "id": "A9m2cPFETpFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = re.sub('Ross', '2016130722', text1)\n",
        "text1"
      ],
      "metadata": {
        "id": "xeygJyZeSl_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"20220509.txt\", \"w\")\n",
        "file.write(text1)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "CHZwCWeTUPgO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}