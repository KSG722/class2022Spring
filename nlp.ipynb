{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KSG722/class2022Spring/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltUO0sYwyGfU"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트를 처리함에 있어서는 tokenization, normalization 등등 여러 가지 방법이 있다\n",
        "\n",
        "이 때 토크나이제이션은 strings(긴 텍스트)을 하나하나하나 거의 단어 수준에서 끊어서 우리가 그걸 들고 있는 것을 토크나이제이션이라고 함. 토큰은 단어, 즉 string을 list 안에다가 단어 단위로 끊어서 넣는 것."
      ],
      "metadata": {
        "id": "6bNQUMTcGMLj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di6xZ08xsgO7"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk : text processing과 관련된 패키지\n",
        "\n",
        "기존에는 sound processing와 관련된 작업이었다면 이제는 text에 대해서 배운다"
      ],
      "metadata": {
        "id": "HdVxanhjGCmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy a file from github\n",
        "import os\n",
        "url = \"https://raw.githubusercontent.com/hsnam95/class2022Spring/main/crime_punishment.txt\"\n",
        "os.system(\"curl \" + url + \" > crime_punishment.txt\")\n",
        "\n",
        "# read a text file in the server\n",
        "file = open(\"crime_punishment.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "btgs9Nt-2Yj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbHEyyNHntcZ"
      },
      "source": [
        "# or copy/pase text here\n",
        "text = 'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently — they’re not fond of rules. \\\n",
        "You can quote them, disagree with them, glorify or vilify them, \\\n",
        "but the only thing you can’t do is ignore them because they change things. \\\n",
        "They push the human race forward, and while some may see them as the crazy ones, we see genius, \\\n",
        "because the ones who are crazy enough to think that they can change the world, are the ones who do.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"tmp.txt\", \"w\")\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "9saTcc9C4Cjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45jV2UYs1GEC"
      },
      "source": [
        "text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFR-cRaahTPy"
      },
      "source": [
        "' '.join(text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq_lPZMHntcb"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "words = word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC1fe7nWF6wN"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\")\n",
        "words = retokenize.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc42Plwx56YS"
      },
      "source": [
        "### Normalization  \n",
        "**Stemming** 어간 추출 대충의 패턴 규칙으로 어미를 잘라내는 것 (사전에 없는 어간 나올 수 있음)\n",
        "\n",
        "**Lemmatization** 표제어(기본 사전형) 추출.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsFfoAr259Fs"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkbgNiPd8BdL"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIh5pYd8f74"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPgIzrjm8_1N"
      },
      "source": [
        "### Stopword"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어. 너무 많이 쓰는 대명사나 조동사, 관사. 너무 자주 나오는 단어들은 분석에 필요가 없으므로 따로 모아 놓은 list."
      ],
      "metadata": {
        "id": "GMKBnPEsGqx5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdM2FaN8ntcc"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "nltk.download('stopwords')\n",
        "print(words)\n",
        "words = [w for w in words if not w in stopwords.words('english')]\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmwXTL0UA5aw"
      },
      "source": [
        "### Collocation, Concordance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "collocation : 연어, 숙어가 대표적인 예시. 예를 들어 take care라고 하면 take와 care가 붙어서 숙어로 나올 가능성이 높다. 혹인 사람 이름. Diddier Drogba에서 디디에와 드록바는 늘 붙어나오는 이름.\n",
        "\n",
        "concordance : 특정 단어 내에서 용례가 어떻게 되는지를 보여줌. wood가 큰 단어 사이에서 어케 쓰이는지."
      ],
      "metadata": {
        "id": "NTPqz4qUG4Ts"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fll4ygxNA3OJ"
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "words = retokenize.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVXlhIrAtmf"
      },
      "source": [
        "nltk.Text(words).collocations()  # default: (num=20, window_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq0wiutwA_au"
      },
      "source": [
        "nltk.Text(words).concordance('Emma', 79, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIAIhXvP_BjU"
      },
      "source": [
        "nltk.Text(words).dispersion_plot([\"Emma\", \"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWYZOFxq_ex2"
      },
      "source": [
        "# Distributional similarity: \n",
        "# find other words which appear in the same contexts as the specified word; \n",
        "# list most similar words first.\n",
        "nltk.Text(words).similar(\"Emma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZihiVSBK_vy7"
      },
      "source": [
        "# Find contexts where the specified words appear; list most frequent common contexts first.\n",
        "nltk.Text(words).common_contexts([\"Emma\", \"she\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8TrCE14vGcT"
      },
      "source": [
        "### Frequency distribution, Frequency plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어떤 단어가 많이 쓰이는지 보는 것이 우리가 분석한 document의 성격이나 특징을 가장 잘 나타내줌. 어떤 다큐먼트가 있을 때 특정한 단어가 많이 보이면 어떤 article인지 보인다. 예를 들어 ball, striker, goalkeeper 등의 단어가 자주 보이면 그 문서는 축구와 관련된 article일 것."
      ],
      "metadata": {
        "id": "mRXb9ZLuHM1m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdY3m6zSBHic"
      },
      "source": [
        "fd = nltk.FreqDist(words).most_common(20)\n",
        "fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tpZThNV-ftv"
      },
      "source": [
        "nltk.Text(words).plot(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSOSzIovvKvE"
      },
      "source": [
        "### Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIcAOAvqntce"
      },
      "source": [
        "nltk.download('words')\n",
        "nltk.corpus.words.words('en')[-20:-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjAy_Ju7ntce"
      },
      "source": [
        "len(nltk.corpus.words.words('en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyIIqwosCRZa"
      },
      "source": [
        "### Extract information (pos tag, named entity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "품사들의 정의와 설명, 약어, 예 등이 있음. 품사가 따라서 품사가 자동으로 태그되어 분석됨."
      ],
      "metadata": {
        "id": "43RBQlsrHjWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **POS tag list**:\n",
        "\n",
        "CC\tcoordinating conjunction \\\n",
        "CD\tcardinal digit \\\n",
        "DT\tdeterminer \\\n",
        "EX\texistential there (like: \"there is\" ... think of it like \"there exists\") \\\n",
        "FW\tforeign word \\\n",
        "IN\tpreposition/subordinating conjunction \\\n",
        "JJ\tadjective\t'big' \\\n",
        "JJR\tadjective, comparative\t'bigger' \\\n",
        "JJS\tadjective, superlative\t'biggest' \\\n",
        "LS\tlist marker\t1) \\\n",
        "MD\tmodal\tcould, will \\\n",
        "NN\tnoun, singular 'desk' \\\n",
        "NNS\tnoun plural\t'desks' \\\n",
        "NNP\tproper noun, singular\t'Harrison' \\\n",
        "NNPS\tproper noun, plural\t'Americans' \\\n",
        "PDT\tpredeterminer\t'all the kids' \\\n",
        "POS\tpossessive ending\tparent's \\\n",
        "PRP\tpersonal pronoun\tI, he, she \\\n",
        "PRP\\$\tpossessive pronoun\tmy, his, hers \\\n",
        "RB\tadverb\tvery, silently, \\\n",
        "RBR\tadverb, comparative\tbetter \\\n",
        "RBS\tadverb, superlative\tbest \\\n",
        "RP\tparticle\tgive up \\\n",
        "TO\tto\tgo 'to' the store. \\\n",
        "UH\tinterjection\terrrrrrrrm \\\n",
        "VB\tverb, base form\ttake \\\n",
        "VBD\tverb, past tense\ttook \\\n",
        "VBG\tverb, gerund/present participle\ttaking \\\n",
        "VBN\tverb, past participle\ttaken \\\n",
        "VBP\tverb, sing. present, non-3d\ttake \\\n",
        "VBZ\tverb, 3rd person sing. present\ttakes \\\n",
        "WDT\twh-determiner\twhich \\\n",
        "WP\twh-pronoun\twho, what \\\n",
        "WP\\$\tpossessive wh-pronoun\twhose \\\n",
        "WRB\twh-abverb\twhere, when \\"
      ],
      "metadata": {
        "id": "NLz0kp4kekaZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VBiObftCVwH"
      },
      "source": [
        "sent = \"I am Jhon from America and would like to go to Starbuck\"\n",
        "words = nltk.word_tokenize(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwKdu36WCewv"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = nltk.pos_tag(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnjGT1HpClE0"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "NE = nltk.ne_chunk(pos)\n",
        "# common Entity types: ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9DEIZ4lXQF"
      },
      "source": [
        "### Wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 입력한 frequency를 기반으로 visualization해주는 것."
      ],
      "metadata": {
        "id": "bt5ua7zOHlD1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jypxOnw9hoyZ"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "\n",
        "wc = WordCloud().generate(text) \n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6xv5ClAl5xk"
      },
      "source": [
        "stopwords = set(STOPWORDS) \n",
        "stopwords.add('unto')\n",
        "wc = WordCloud(stopwords = stopwords).generate(text) \n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrVGVc0X9j7r"
      },
      "source": [
        "### Regular expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKgoQFI_cG-"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U2pS-NL9p38"
      },
      "source": [
        "'''       Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\n",
        "\n",
        ".\t        Wildcard, matches any character\n",
        "^abc\t    Matches some pattern abc at the start of a string\n",
        "abc$\t    Matches some pattern abc at the end of a string\n",
        "[abc]\t    Matches one of a set of characters\n",
        "[^abc]    Matches anything but a set of characters\n",
        "[A-Z0-9]\tMatches one of a range of characters\n",
        "ed|ing|s\tMatches one of the specified strings (disjunction)\n",
        "*\t        Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
        "+\t        One or more of previous item, e.g. a+, [a-z]+\n",
        "?\t        Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
        "{n}\t      Exactly n repeats where n is a non-negative integer\n",
        "{n,}\t    At least n repeats\n",
        "{,n}\t    No more than n repeats\n",
        "{m,n}\t    At least m and no more than n repeats\n",
        "a(b|c)+\t  Parentheses that indicate the scope of the operators\n",
        "(...)     Matches whatever regular expression is inside the parentheses\n",
        "\\d\n",
        "Matches any decimal digit; this is equivalent to the class [0-9].\n",
        "\\D\n",
        "Matches any non-digit character; this is equivalent to the class [^0-9].\n",
        "\\s\n",
        "Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v].\n",
        "\\S\n",
        "Matches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\f\\v].\n",
        "\\w\n",
        "Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
        "\\W\n",
        "Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp3_Dm9Q_tNQ"
      },
      "source": [
        "engdict = nltk.corpus.words.words('en')\n",
        "\n",
        "result = [w for w in engdict if re.search('ed$', w)]\n",
        "# result = [w for w in engdict if re.search('^..j..t..$', w)]\n",
        "# result = [w for w in engdict if re.search('^[ghi][mno][jlk][def]$', w)]\n",
        "# result = [w for w in engdict if re.search('^[ah]+$', w)][:10]\n",
        "print(result[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1a5mQYj4hwn"
      },
      "source": [
        "nltk.download('treebank')\n",
        "wsj = nltk.corpus.treebank.words()\n",
        "\n",
        "result = [w for w in wsj if re.search('(ed|ing)$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
        "# result = [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
        "# result = [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
        "\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}